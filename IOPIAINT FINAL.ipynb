{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remphan1618/liteOutInpainta/blob/main/IOPIAINT%20FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üé® Model Lister\n",
        "\n",
        "\n",
        "\n",
        "from huggingface_hub import list_models\n",
        "from huggingface_hub.utils import HfHubHTTPError # Import for better error handling\n",
        "\n",
        "# Define the filters\n",
        "try:\n",
        "    models_generator = list_models(\n",
        "        pipeline_tag=\"text-to-image\",\n",
        "        library=\"diffusers\",\n",
        "        search=\"inpaint\",\n",
        "        sort=\"lastModified\", # Sort by last update time\n",
        "        direction=-1,       # -1 for descending (newest first)\n",
        "    )\n",
        "\n",
        "    # Extract the repo_ids\n",
        "    inpaint_model_ids = [model.modelId for model in models_generator]\n",
        "\n",
        "    # --- Print model IDs 10 per line, space-separated ---\n",
        "    print(\"List of Inpaint Model IDs (10 per line, space-separated):\")\n",
        "    if inpaint_model_ids:\n",
        "        # Print model IDs in blocks of 10 per line\n",
        "        for i in range(0, len(inpaint_model_ids), 10):\n",
        "            # Get a slice of 10 model IDs\n",
        "            batch = inpaint_model_ids[i:i + 10]\n",
        "            # Join them with a space and print\n",
        "            print(\" \".join(batch))\n",
        "    else:\n",
        "        print(\"No models found matching the criteria.\")\n",
        "\n",
        "    # --- Print model IDs in chunks, comma-separated with trailing comma ---\n",
        "    comma_chunk_size = 5 # You can change this to 10 if you prefer\n",
        "    print(f\"\\nComma-separated list ({comma_chunk_size} per line, with space and trailing comma):\")\n",
        "    if inpaint_model_ids:\n",
        "        # Print model IDs in blocks of 'comma_chunk_size' per line\n",
        "        for i in range(00, len(inpaint_model_ids), comma_chunk_size): # Start from 0\n",
        "            # Get a slice of 'comma_chunk_size' model IDs\n",
        "            batch = inpaint_model_ids[i:i + comma_chunk_size]\n",
        "            # Join them with a comma and a space\n",
        "            line = \", \".join(batch)\n",
        "\n",
        "            # Check if this is the last batch\n",
        "            is_last_batch = (i + comma_chunk_size) >= len(inpaint_model_ids)\n",
        "\n",
        "            if not is_last_batch:\n",
        "                # If not the last batch, print the line followed by a comma and newline\n",
        "                print(line + \",\") # Add trailing comma\n",
        "            else:\n",
        "                # If it is the last batch, just print the line\n",
        "                print(line)\n",
        "    else:\n",
        "         print(f\"\\nNo models found to create a comma-separated list ({comma_chunk_size} per line).\")\n",
        "\n",
        "\n",
        "except HfHubHTTPError as e:\n",
        "    print(f\"An HTTP error occurred: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rliTHBwEtQ07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d91cb81-744a-4a61-ed70-810f66146a8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An HTTP error occurred: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models?filter=diffusers&pipeline_tag=text-to-image&search=inpaint&sort=lastModified&direction=-1 (Request ID: Root=1-6811643c-7a49788d6d3e8bb04f72275a;5dcd2abd-293e-489c-9284-965ff5b0936b)\n",
            "\n",
            "Invalid credentials in Authorization header\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üé® All-in-One IOpaint with Ngrok\n",
        "\n",
        "#@markdown ## üîë Ngrok Configuration\n",
        "#@markdown Enter your [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken) below:\n",
        "ngrok_token = \"2tjxIXifSaGR3dMhkvhk6sZqbGo_6ZfBZLZHMbtAjfRmfoDW5\" #@param {type:\"string\"}\n",
        "save_token = True #@param {type:\"boolean\"}\n",
        "token_file = \"/content/ngrok_token\" #@param {type:\"string\"}\n",
        "run_without_ngrok = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## üì¶ Model Selection\n",
        "#@markdown ### Checkpoint Models (comma-separated list)\n",
        "checkpoint_models = \"lama, sam2_base, gfpgan\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Diffusers Models (comma-separated list of HF repos)\n",
        "diffusers_models = \"\" #@param {type:\"string\"}\n",
        "use_fp16_for_diffusers = True #@param {type:\"boolean\"}\n",
        "pre_download_diffusers = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## üöÄ IOpaint Options\n",
        "selected_model = \"lama\" #@param [\"lama\", \"mat\", \"migan\", \"sdxl_turbo\", \"urpm_inpaint_sdxl\"]\n",
        "enable_interactive_seg = True #@param {type:\"boolean\"}\n",
        "enable_remove_bg = True #@param {type:\"boolean\"}\n",
        "enable_realesrgan = True #@param {type:\"boolean\"}\n",
        "enable_gfpgan = True #@param {type:\"boolean\"}\n",
        "disable_nsfw = True #@param {type:\"boolean\"}\n",
        "port = 1111 #@param {type:\"integer\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "# Current timestamp\n",
        "print(f\"Current Date and Time: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
        "print(f\"Current User: remphan1618\")\n",
        "\n",
        "# Define base directories - Using /content/ for user accessibility\n",
        "BASE_MODEL_DIR = \"/content/iopaint/models\"\n",
        "TORCH_HUB_DIR = os.path.join(BASE_MODEL_DIR, \"torch/hub/checkpoints\")\n",
        "os.makedirs(BASE_MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(TORCH_HUB_DIR, exist_ok=True)\n",
        "\n",
        "# ========== INSTALL REQUIRED PACKAGES ==========\n",
        "print(\"\\nüì¶ Installing required packages...\")\n",
        "\n",
        "# Install packages one by one to better handle failures\n",
        "def install_package(package):\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        print(f\"‚úÖ Successfully installed {package}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to install {package}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Critical packages first\n",
        "install_package(\"iopaint\")\n",
        "\n",
        "# Install pyngrok only if we're going to use it\n",
        "if not run_without_ngrok:\n",
        "    install_package(\"pyngrok\")\n",
        "\n",
        "# Other required/optional packages\n",
        "install_package(\"huggingface_hub\")\n",
        "if enable_gfpgan:\n",
        "    install_package(\"gfpgan\")\n",
        "if enable_remove_bg:\n",
        "    install_package(\"rembg\")\n",
        "    install_package(\"onnxruntime\")\n",
        "\n",
        "# Create symlink from default location to our accessible location\n",
        "default_model_dir = \"/root/.cache/iopaint/models\"\n",
        "os.makedirs(os.path.dirname(default_model_dir), exist_ok=True)\n",
        "if os.path.exists(default_model_dir):\n",
        "    if os.path.islink(default_model_dir):\n",
        "        os.unlink(default_model_dir)\n",
        "    else:\n",
        "        import shutil\n",
        "        print(f\"Moving existing models from {default_model_dir} to {BASE_MODEL_DIR}...\")\n",
        "        for item in os.listdir(default_model_dir):\n",
        "            src = os.path.join(default_model_dir, item)\n",
        "            dst = os.path.join(BASE_MODEL_DIR, item)\n",
        "            if not os.path.exists(dst):\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        shutil.rmtree(default_model_dir)\n",
        "\n",
        "# Create symlink\n",
        "os.symlink(BASE_MODEL_DIR, default_model_dir)\n",
        "print(f\"‚úÖ Models directory set to {BASE_MODEL_DIR} (accessible via /content/)\")\n",
        "print(f\"‚úÖ Created symlink from {default_model_dir} to {BASE_MODEL_DIR}\")\n",
        "\n",
        "# ========== HANDLE NGROK TOKEN ==========\n",
        "use_ngrok = False\n",
        "if not run_without_ngrok:\n",
        "    # Load token from file if exists\n",
        "    if not ngrok_token and os.path.exists(token_file):\n",
        "        with open(token_file, 'r') as f:\n",
        "            ngrok_token = f.read().strip()\n",
        "        print(f\"‚úÖ Loaded Ngrok token from {token_file}\")\n",
        "\n",
        "    # Save token if requested\n",
        "    if ngrok_token and save_token:\n",
        "        with open(token_file, 'w') as f:\n",
        "            f.write(ngrok_token)\n",
        "        os.chmod(token_file, 0o600)  # Set permissions to user-only\n",
        "        print(f\"‚úÖ Saved Ngrok token to {token_file}\")\n",
        "\n",
        "    if not ngrok_token:\n",
        "        print(\"‚ö†Ô∏è WARNING: No Ngrok token provided.\")\n",
        "        print(\"Will continue without Ngrok - you'll only be able to access IOpaint through Colab's port forwarding.\")\n",
        "    else:\n",
        "        use_ngrok = True\n",
        "else:\n",
        "    print(\"üìå Running without Ngrok as requested.\")\n",
        "\n",
        "# ========== CHECKPOINT MODEL DOWNLOADER ==========\n",
        "# Checkpoint models database\n",
        "CHECKPOINT_MODELS = {\n",
        "    \"lama\": {\n",
        "        \"url\": \"https://github.com/Sanster/models/releases/download/add_big_lama/big-lama.pt\",\n",
        "        \"filename\": \"big-lama.pt\",\n",
        "        \"size_mb\": 196,\n",
        "        \"description\": \"Default fast inpainting model\"\n",
        "    },\n",
        "    \"mat\": {\n",
        "        \"url\": \"https://github.com/Sanster/models/releases/download/mat/Places_512_FullData_G.pth\",\n",
        "        \"filename\": \"Places_512_FullData_G.pth\",\n",
        "        \"size_mb\": 675,\n",
        "        \"description\": \"Better quality than LaMa but slower\"\n",
        "    },\n",
        "    \"sam2_base\": {\n",
        "        \"url\": \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\",\n",
        "        \"filename\": \"sam2_hiera_base_plus.pt\",\n",
        "        \"size_mb\": 309,\n",
        "        \"description\": \"SAM2 model for interactive segmentation\"\n",
        "    },\n",
        "    \"gfpgan\": {\n",
        "        \"url\": \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\",\n",
        "        \"filename\": \"GFPGANv1.3.pth\",\n",
        "        \"size_mb\": 332,\n",
        "        \"description\": \"Face restoration model\"\n",
        "    },\n",
        "    \"migan\": {\n",
        "        \"url\": \"https://github.com/Sanster/models/releases/download/migan/migan_patch.pt\",\n",
        "        \"filename\": \"migan_patch.pt\",\n",
        "        \"size_mb\": 395,\n",
        "        \"description\": \"Another good inpainting model\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def download_checkpoint(url, dest_path):\n",
        "    \"\"\"Download a file with progress bar\"\"\"\n",
        "    if os.path.exists(dest_path):\n",
        "        file_size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n",
        "        print(f\"‚úÖ Already exists: {os.path.basename(dest_path)} ({file_size_mb:.1f} MB)\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "\n",
        "            with open(dest_path, 'wb') as f, tqdm(\n",
        "                desc=os.path.basename(dest_path),\n",
        "                total=total_size,\n",
        "                unit='B',\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    size = f.write(chunk)\n",
        "                    bar.update(size)\n",
        "\n",
        "        if os.path.exists(dest_path) and os.path.getsize(dest_path) > 0:\n",
        "            size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n",
        "            print(f\"‚úÖ Downloaded: {os.path.basename(dest_path)} ({size_mb:.1f} MB)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to download: {dest_path}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Process checkpoint models\n",
        "if checkpoint_models:\n",
        "    model_list = [m.strip() for m in checkpoint_models.split(\",\") if m.strip()]\n",
        "    print(f\"\\nüì¶ Downloading {len(model_list)} checkpoint models: {', '.join(model_list)}\")\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if model_name in CHECKPOINT_MODELS:\n",
        "            print(f\"\\nüì• Downloading {model_name}: {CHECKPOINT_MODELS[model_name]['description']}\")\n",
        "            dest_path = os.path.join(TORCH_HUB_DIR, CHECKPOINT_MODELS[model_name]['filename'])\n",
        "            download_checkpoint(CHECKPOINT_MODELS[model_name]['url'], dest_path)\n",
        "        else:\n",
        "            print(f\"‚ùå Unknown model: {model_name}\")\n",
        "\n",
        "# ========== DIFFUSERS MODEL DOWNLOADER ==========\n",
        "def sanitize_name(repo_id):\n",
        "    \"\"\"Create a valid model name from repository ID\"\"\"\n",
        "    name = repo_id.split(\"/\")[-1].lower()\n",
        "    name = name.replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\")\n",
        "    if len(name) > 30:\n",
        "        name = name[:30]\n",
        "    return name\n",
        "\n",
        "def setup_diffusers_model(repo_id, use_fp16=False, pre_download=False):\n",
        "    \"\"\"Set up a diffusers model by creating model_info.json\"\"\"\n",
        "    model_name = sanitize_name(repo_id)\n",
        "    model_dir = os.path.join(BASE_MODEL_DIR, model_name)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Create model_info.json\n",
        "    config = {\n",
        "        \"name\": model_name,\n",
        "        \"repo_id\": repo_id,\n",
        "        \"subfolder\": None,\n",
        "        \"type\": \"diffusers\",\n",
        "        \"fp16\": use_fp16,\n",
        "        \"revision\": \"main\",\n",
        "        \"use_checkpoint\": False  # Force diffusers format only\n",
        "    }\n",
        "\n",
        "    config_path = os.path.join(model_dir, \"model_info.json\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Set up model: {model_name}\")\n",
        "    print(f\"  ‚Ä¢ Repository: {repo_id}\")\n",
        "    print(f\"  ‚Ä¢ Using FP16: {'Yes' if use_fp16 else 'No'}\")\n",
        "\n",
        "    # Optionally pre-download the model\n",
        "    if pre_download:\n",
        "        try:\n",
        "            print(f\"  ‚Ä¢ Pre-downloading diffusers components...\")\n",
        "\n",
        "            # Import huggingface_hub\n",
        "            from huggingface_hub import snapshot_download\n",
        "\n",
        "            # Only download diffusers components\n",
        "            allow_patterns = [\n",
        "                \"*.json\",\n",
        "                \"scheduler/**\",\n",
        "                \"text_encoder/**\",\n",
        "                \"text_encoder_2/**\",\n",
        "                \"tokenizer/**\",\n",
        "                \"unet/**\",\n",
        "                \"vae/**\",\n",
        "                \"safety_checker/**\",\n",
        "                \"feature_extractor/**\"\n",
        "            ]\n",
        "\n",
        "            # Skip checkpoint files\n",
        "            ignore_patterns = [\"*.safetensors\", \"*.ckpt\", \"*.bin\", \"*.pt\"]\n",
        "\n",
        "            # Download with progress tracking\n",
        "            print(f\"    ‚è≥ Downloading from {repo_id}...\")\n",
        "            snapshot_download(\n",
        "                repo_id=repo_id,\n",
        "                local_dir=model_dir,\n",
        "                allow_patterns=allow_patterns,\n",
        "                ignore_patterns=ignore_patterns\n",
        "            )\n",
        "            print(f\"    ‚úÖ Download complete!\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error pre-downloading: {str(e)}\")\n",
        "            print(f\"    ‚ÑπÔ∏è Model will download when first used in IOpaint.\")\n",
        "    else:\n",
        "        print(f\"  ‚Ä¢ Model will download when first used in IOpaint\")\n",
        "\n",
        "    return model_name\n",
        "\n",
        "# Process diffusers models\n",
        "configured_diffusers = []\n",
        "if diffusers_models:\n",
        "    model_repos = [m.strip() for m in diffusers_models.split(\",\") if m.strip()]\n",
        "    print(f\"\\nüß© Setting up {len(model_repos)} diffusers models\")\n",
        "\n",
        "    for repo_id in model_repos:\n",
        "        print(f\"\\nüì• Processing: {repo_id}\")\n",
        "        model_name = setup_diffusers_model(repo_id, use_fp16_for_diffusers, pre_download_diffusers)\n",
        "        configured_diffusers.append(model_name)\n",
        "        time.sleep(0.5)  # Small delay between setups\n",
        "\n",
        "# ========== NGROK & IOPAINT LAUNCHER ==========\n",
        "# Set up Ngrok if we're using it\n",
        "url = None\n",
        "if use_ngrok:\n",
        "    try:\n",
        "        print(\"\\nüîÑ Setting up Ngrok...\")\n",
        "        from pyngrok import ngrok, conf, exception\n",
        "\n",
        "        # Explicitly clear any previous authtoken\n",
        "        conf.get_default().auth_token = None\n",
        "\n",
        "        # Set auth token\n",
        "        print(f\"Setting Ngrok auth token...\")\n",
        "        ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "        # Verify the token by getting API info\n",
        "        try:\n",
        "            print(\"Verifying Ngrok auth token...\")\n",
        "            # Try to use the API to verify the token\n",
        "            tunnels = ngrok.get_tunnels()\n",
        "            print(\"‚úÖ Ngrok token is valid\")\n",
        "        except exception.PyngrokNgrokError as e:\n",
        "            print(f\"‚ùå Ngrok authentication failed: {str(e)}\")\n",
        "            print(\"üîÑ Falling back to local-only access\")\n",
        "            use_ngrok = False\n",
        "\n",
        "        if use_ngrok:\n",
        "            # Create tunnel\n",
        "            print(f\"Creating Ngrok tunnel on port {port}...\")\n",
        "            url = ngrok.connect(addr=port, bind_tls=True)\n",
        "            print(f\"‚úÖ Ngrok tunnel created: {url}\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Failed to import pyngrok. Falling back to local-only access.\")\n",
        "        use_ngrok = False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up Ngrok: {str(e)}\")\n",
        "        print(\"üîÑ Falling back to local-only access\")\n",
        "        use_ngrok = False\n",
        "\n",
        "# Display URL info based on what we're using\n",
        "if use_ngrok and url:\n",
        "    # Display clickable Ngrok link\n",
        "    display(HTML(f'''\n",
        "    <div style=\"background-color: #f0f5ff; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
        "      <h3>üé® IOpaint is Ready!</h3>\n",
        "      <p><b>Access URL:</b> <a href=\"{url}\" target=\"_blank\">{url}</a></p>\n",
        "      <p><b>Models folder:</b> <code>/content/iopaint/models</code> (accessible in file explorer)</p>\n",
        "      <p><b>Selected model:</b> <code>{selected_model}</code></p>\n",
        "    </div>\n",
        "    '''))\n",
        "else:\n",
        "    # Display info about local access\n",
        "    display(HTML(f'''\n",
        "    <div style=\"background-color: #f0f5ff; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
        "      <h3>üé® IOpaint is Running Locally</h3>\n",
        "      <p><b>Access method:</b> Use the \"Connect\" button at the top right of the Colab window, then select \"Connect to local runtime\"</p>\n",
        "      <p><b>Models folder:</b> <code>/content/iopaint/models</code> (accessible in file explorer)</p>\n",
        "      <p><b>Selected model:</b> <code>{selected_model}</code></p>\n",
        "      <p><b>Note:</b> Your IOpaint instance will be available at <a href=\"http://localhost:{port}\" target=\"_blank\">http://localhost:{port}</a> after connecting to the local runtime.</p>\n",
        "    </div>\n",
        "    '''))\n",
        "\n",
        "# Build IOPaint command\n",
        "cmd = [\n",
        "    'iopaint', 'start',\n",
        "    '--model', selected_model,\n",
        "    '--device', 'cuda',\n",
        "    '--port', str(port),\n",
        "    '--host', '0.0.0.0'\n",
        "]\n",
        "\n",
        "# Add optional parameters\n",
        "if enable_interactive_seg:\n",
        "    cmd.extend(['--enable-interactive-seg', '--interactive-seg-device', 'cuda'])\n",
        "\n",
        "if enable_remove_bg:\n",
        "    cmd.append('--enable-remove-bg')\n",
        "\n",
        "if enable_realesrgan:\n",
        "    cmd.extend(['--enable-realesrgan', '--realesrgan-device', 'cuda'])\n",
        "\n",
        "if enable_gfpgan:\n",
        "    cmd.extend(['--enable-gfpgan', '--gfpgan-device', 'cuda'])\n",
        "\n",
        "if disable_nsfw:\n",
        "    cmd.append('--disable-nsfw-checker')\n",
        "\n",
        "print(\"\\nüöÄ Launching IOPaint with command:\")\n",
        "print(' '.join(cmd))\n",
        "print(\"\\n‚è≥ Please wait until you see 'Application startup complete'...\\n\")\n",
        "\n",
        "# Run the process and keep it alive\n",
        "import asyncio\n",
        "\n",
        "async def run_process():\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    async def read_stream(stream):\n",
        "        while True:\n",
        "            line = await stream.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            print(line.decode('utf-8').strip())\n",
        "\n",
        "    await asyncio.gather(\n",
        "        read_stream(process.stdout),\n",
        "        read_stream(process.stderr)\n",
        "    )\n",
        "\n",
        "# Run the process\n",
        "try:\n",
        "    await run_process()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping on user request...\")\n",
        "finally:\n",
        "    # Clean up Ngrok tunnels if we were using them\n",
        "    if use_ngrok:\n",
        "        try:\n",
        "            print(\"Cleaning up Ngrok tunnels...\")\n",
        "            ngrok.kill()\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WappSzfitUuW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19ddaaf7-c77f-4db5-ff0d-2a633e60b75f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Date and Time: 2025-04-29 23:38:29 UTC\n",
            "Current User: remphan1618\n",
            "\n",
            "üì¶ Installing required packages...\n",
            "Installing iopaint...\n",
            "‚úÖ Successfully installed iopaint\n",
            "Installing pyngrok...\n",
            "‚úÖ Successfully installed pyngrok\n",
            "Installing huggingface_hub...\n",
            "‚úÖ Successfully installed huggingface_hub\n",
            "Installing gfpgan...\n",
            "‚úÖ Successfully installed gfpgan\n",
            "Installing rembg...\n",
            "‚úÖ Successfully installed rembg\n",
            "Installing onnxruntime...\n",
            "‚úÖ Successfully installed onnxruntime\n",
            "‚úÖ Models directory set to /content/iopaint/models (accessible via /content/)\n",
            "‚úÖ Created symlink from /root/.cache/iopaint/models to /content/iopaint/models\n",
            "‚úÖ Saved Ngrok token to /content/ngrok_token\n",
            "\n",
            "üì¶ Downloading 3 checkpoint models: lama, sam2_base, gfpgan\n",
            "\n",
            "üì• Downloading lama: Default fast inpainting model\n",
            "‚úÖ Already exists: big-lama.pt (196.1 MB)\n",
            "\n",
            "üì• Downloading sam2_base: SAM2 model for interactive segmentation\n",
            "‚úÖ Already exists: sam2_hiera_base_plus.pt (308.5 MB)\n",
            "\n",
            "üì• Downloading gfpgan: Face restoration model\n",
            "‚úÖ Already exists: GFPGANv1.3.pth (332.5 MB)\n",
            "\n",
            "üîÑ Setting up Ngrok...\n",
            "Setting Ngrok auth token...\n",
            "Verifying Ngrok auth token...\n",
            "‚úÖ Ngrok token is valid\n",
            "Creating Ngrok tunnel on port 1111...\n",
            "‚úÖ Ngrok tunnel created: NgrokTunnel: \"https://b1d3-34-143-228-87.ngrok-free.app\" -> \"http://localhost:1111\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"background-color: #f0f5ff; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
              "      <h3>üé® IOpaint is Ready!</h3>\n",
              "      <p><b>Access URL:</b> <a href=\"NgrokTunnel: \"https://b1d3-34-143-228-87.ngrok-free.app\" -> \"http://localhost:1111\"\" target=\"_blank\">NgrokTunnel: \"https://b1d3-34-143-228-87.ngrok-free.app\" -> \"http://localhost:1111\"</a></p>\n",
              "      <p><b>Models folder:</b> <code>/content/iopaint/models</code> (accessible in file explorer)</p>\n",
              "      <p><b>Selected model:</b> <code>lama</code></p>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Launching IOPaint with command:\n",
            "iopaint start --model lama --device cuda --port 1111 --host 0.0.0.0 --enable-interactive-seg --interactive-seg-device cuda --enable-remove-bg --enable-realesrgan --realesrgan-device cuda --enable-gfpgan --gfpgan-device cuda --disable-nsfw-checker\n",
            "\n",
            "‚è≥ Please wait until you see 'Application startup complete'...\n",
            "\n",
            "2025-04-29 23:38:46.407 | INFO     | iopaint.runtime:setup_model_dir:81 - Model directory: /root/.cache\n",
            "- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "- Python version: 3.11.12\n",
            "- torch: 2.6.0+cu124\n",
            "- torchvision: 0.21.0+cu124\n",
            "- Pillow: 9.5.0\n",
            "- diffusers: 0.27.2\n",
            "- transformers: 4.48.3\n",
            "- opencv-python: 4.11.0.86\n",
            "- accelerate: 1.6.0\n",
            "- iopaint: 1.6.0\n",
            "- rembg: 2.0.65\n",
            "- onnxruntime: 1.21.1\n",
            "\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745969931.613958   14371 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745969931.620533   14371 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/iopaint/model/ldm.py:279: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "@torch.cuda.amp.autocast()\n",
            "[W429 23:38:55.441360336 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
            "2025-04-29 23:38:55.095 | INFO     | iopaint.plugins:build_plugins:33 - Initialize InteractiveSeg plugin\n",
            "2025-04-29 23:38:55.095 | INFO     | iopaint.plugins.interactive_seg:_init_session:96 - SegmentAnything model path: /root/.cache/torch/hub/checkpoints/sam2.1_hiera_tiny.pt\n",
            "2025-04-29 23:38:56.220 | INFO     | iopaint.plugins:build_plugins:39 - Initialize RemoveBG plugin\n",
            "2025-04-29 23:38:57.843 | INFO     | iopaint.plugins:build_plugins:47 - Initialize RealESRGAN plugin: RealESRGANModel.realesr_general_x4v3, Device.cuda\n",
            "2025-04-29 23:38:57.844 | INFO     | iopaint.plugins.realesrgan:_init_model:438 - RealESRGAN model path: /root/.cache/torch/hub/checkpoints/realesr-general-x4v3.pth\n",
            "2025-04-29 23:38:57.885 | INFO     | iopaint.plugins:build_plugins:57 - Initialize GFPGAN plugin\n",
            "2025-04-29 23:38:57.885 | INFO     | iopaint.plugins:build_plugins:59 - Use realesrgan as GFPGAN background upscaler\n",
            "2025-04-29 23:38:57.890 | INFO     | iopaint.plugins.gfpgan_plugin:__init__:21 - GFPGAN model path: /root/.cache/torch/hub/checkpoints/GFPGANv1.4.pth\n",
            "2025-04-29 23:38:59.932 | INFO     | iopaint.model_manager:init_model:47 - Loading model: lama\n",
            "2025-04-29 23:38:59.932 | INFO     | iopaint.helper:load_jit_model:107 - Loading model from: /root/.cache/torch/hub/checkpoints/big-lama.pt\n",
            "INFO:     Started server process [14371]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:1111 (Press CTRL+C to quit)\n",
            "{\n",
            "\"host\": \"0.0.0.0\",\n",
            "\"port\": 1111,\n",
            "\"inbrowser\": false,\n",
            "\"model\": \"lama\",\n",
            "\"no_half\": false,\n",
            "\"low_mem\": false,\n",
            "\"cpu_offload\": false,\n",
            "\"disable_nsfw_checker\": true,\n",
            "\"local_files_only\": false,\n",
            "\"cpu_textencoder\": false,\n",
            "\"device\": \"cuda\",\n",
            "\"input\": null,\n",
            "\"mask_dir\": null,\n",
            "\"output_dir\": null,\n",
            "\"quality\": 100,\n",
            "\"enable_interactive_seg\": true,\n",
            "\"interactive_seg_model\": \"sam2_1_tiny\",\n",
            "\"interactive_seg_device\": \"cuda\",\n",
            "\"enable_remove_bg\": true,\n",
            "\"remove_bg_device\": \"cpu\",\n",
            "\"remove_bg_model\": \"briaai/RMBG-1.4\",\n",
            "\"enable_anime_seg\": false,\n",
            "\"enable_realesrgan\": true,\n",
            "\"realesrgan_device\": \"cuda\",\n",
            "\"realesrgan_model\": \"realesr-general-x4v3\",\n",
            "\"enable_gfpgan\": true,\n",
            "\"gfpgan_device\": \"cuda\",\n",
            "\"enable_restoreformer\": false,\n",
            "\"restoreformer_device\": \"cpu\"\n",
            "}\n",
            "INFO:     203.211.75.14:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /assets/index-OeSab6Ny.js HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /assets/index-chrgyh9P.css HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /socket.io/?EIO=4&transport=polling&t=PQ3xSB9 HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /api/v1/server-config HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /api/v1/inputimage HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /api/v1/model HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /api/v1/server-config HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"POST /socket.io/?EIO=4&transport=polling&t=PQ3xSHw&sid=SgQJaW5VB35zYebxAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /socket.io/?EIO=4&transport=polling&t=PQ3xSHx&sid=SgQJaW5VB35zYebxAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     ('203.211.75.14', 0) - \"WebSocket /socket.io/?EIO=4&transport=websocket&sid=SgQJaW5VB35zYebxAAAA\" [accepted]\n",
            "INFO:     connection open\n",
            "INFO:     203.211.75.14:0 - \"GET /favicon.ico HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /socket.io/?EIO=4&transport=polling&t=PQ3xSOl&sid=SgQJaW5VB35zYebxAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"GET /socket.io/?EIO=4&transport=polling&t=PQ3xSVX&sid=SgQJaW5VB35zYebxAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     203.211.75.14:0 - \"POST /api/v1/gen-info HTTP/1.1\" 200 OK\n",
            "Cleaning up Ngrok tunnels...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-732c1bbaad63>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;31m# Run the process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mrun_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stopping on user request...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-732c1bbaad63>\u001b[0m in \u001b[0;36mrun_process\u001b[0;34m()\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     await asyncio.gather(\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0mread_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mread_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-732c1bbaad63>\u001b[0m in \u001b[0;36mread_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/streams.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mseplen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaduntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncompleteReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/streams.py\u001b[0m in \u001b[0;36mreaduntil\u001b[0;34m(self, separator)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;31m# _wait_for_data() will resume reading if stream was paused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'readuntil'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/streams.py\u001b[0m in \u001b[0;36m_wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}