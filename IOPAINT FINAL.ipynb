{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remphan1618/liteOutInpainta/blob/main/IOPAINT%20FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hugging Face Login & Model Lister\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import huggingface_hub\n",
        "import os\n",
        "from huggingface_hub.utils import HfHubHTTPError # Import for better error handling\n",
        "\n",
        "# --- Section 1: Token Input Widget ---\n",
        "# We need to create the widget *outside* any conditional logic\n",
        "# so it's displayed when the cell is run the first time.\n",
        "\n",
        "# Display instructions\n",
        "print(\"Step 1: Please enter your Hugging Face token in the box below.\")\n",
        "print(\"You can get your token from your Hugging Face account settings:\")\n",
        "print(\"https://huggingface.co/settings/tokens\")\n",
        "print(\"\\nStep 2: After pasting the token, **run this cell again**.\")\n",
        "\n",
        "\n",
        "# Check if a widget with this description already exists to avoid displaying duplicates\n",
        "# This is a common pattern in notebooks when re-running cells\n",
        "try:\n",
        "    # Attempt to find the widget by its description\n",
        "    token_input # Check if the variable exists from a previous run\n",
        "    print(\"\\nUsing existing HF Token input box.\")\n",
        "except NameError:\n",
        "    # Create the widget if it doesn't exist\n",
        "    token_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Paste your HF token here',\n",
        "        description='HF Token:',\n",
        "        disabled=False,\n",
        "        password=True # Masks the input for security\n",
        "    )\n",
        "    # Display the widget only if we just created it\n",
        "    display(token_input)\n",
        "\n",
        "\n",
        "# --- Section 2: Login and Model Listing (Runs on cell execution) ---\n",
        "\n",
        "# Access the token value from the widget *when the cell is executed*\n",
        "user_token = token_input.value.strip() # Use strip() to remove potential leading/trailing whitespace\n",
        "\n",
        "# Check if a token was actually provided in the input box\n",
        "if user_token:\n",
        "    print(\"\\nAttempting to log in to Hugging Face...\")\n",
        "    try:\n",
        "        # Use huggingface_hub.login() which handles setting up the environment\n",
        "        # We pass the token directly from the widget value\n",
        "        huggingface_hub.login(token=user_token)\n",
        "        print(\"Successfully logged in to Hugging Face.\")\n",
        "\n",
        "        # --- Section 3: Model Listing (Only runs after successful login) ---\n",
        "        print(\"\\nProceeding to list models...\")\n",
        "\n",
        "        # Define the filters\n",
        "        try:\n",
        "            # This call will now use the credentials set up by huggingface_hub.login()\n",
        "            models_generator = list_models(\n",
        "                pipeline_tag=\"text-to-image\",\n",
        "                library=\"diffusers\",\n",
        "                search=\"inpaint\",\n",
        "                sort=\"lastModified\", # Sort by last update time\n",
        "                direction=-1,       # -1 for descending (newest first)\n",
        "                # The token parameter is usually not needed here if login() was successful,\n",
        "                # but you could explicitly pass it: token=user_token\n",
        "            )\n",
        "\n",
        "            # Extract the repo_ids\n",
        "            # Convert generator to a list if needed for repeated access,\n",
        "            # but for your printing logic, iterating the generator directly works.\n",
        "            inpaint_model_ids = [model.modelId for model in models_generator]\n",
        "\n",
        "\n",
        "            # --- Print model IDs 10 per line, space-separated ---\n",
        "            print(\"\\nList of Inpaint Model IDs (10 per line, space-separated):\")\n",
        "            if inpaint_model_ids:\n",
        "                # Print model IDs in blocks of 10 per line\n",
        "                for i in range(0, len(inpaint_model_ids), 10):\n",
        "                    # Get a slice of 10 model IDs\n",
        "                    batch = inpaint_model_ids[i:i + 10]\n",
        "                    # Join them with a space and print\n",
        "                    print(\" \".join(batch))\n",
        "            else:\n",
        "                print(\"No models found matching the criteria.\")\n",
        "\n",
        "            # --- Print model IDs in chunks, comma-separated with trailing comma ---\n",
        "            comma_chunk_size = 5 # You can change this to 10 if you prefer\n",
        "            print(f\"\\nComma-separated list ({comma_chunk_size} per line, with space and trailing comma):\")\n",
        "            if inpaint_model_ids:\n",
        "                # Print model IDs in blocks of 'comma_chunk_size' per line\n",
        "                for i in range(0, len(inpaint_model_ids), comma_chunk_size):\n",
        "                    # Get a slice of 'comma_chunk_size' model IDs\n",
        "                    batch = inpaint_model_ids[i:i + comma_chunk_size]\n",
        "                    # Join them with a comma and a space\n",
        "                    line = \", \".join(batch)\n",
        "\n",
        "                    # Check if this is the last batch\n",
        "                    is_last_batch = (i + comma_chunk_size) >= len(inpaint_model_ids)\n",
        "\n",
        "                    if not is_last_batch:\n",
        "                        # If not the last batch, print the line followed by a comma and newline\n",
        "                        print(line + \",\") # Add trailing comma\n",
        "                    else:\n",
        "                        # If it is the last batch, just print the line\n",
        "                        print(line)\n",
        "            else:\n",
        "                 print(f\"\\nNo models found to create a comma-separated list ({comma_chunk_size} per line).\")\n",
        "\n",
        "\n",
        "        except HfHubHTTPError as e:\n",
        "            print(f\"\\nAn HTTP error occurred during model listing: {e}\")\n",
        "            print(\"Please check your filters and ensure the token has read permissions.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn unexpected error occurred during model listing: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch login errors specifically\n",
        "        print(f\"\\nHugging Face login failed: {e}\")\n",
        "        print(\"Please check your token and try again.\")\n",
        "        print(\"Make sure the token has sufficient permissions (at least 'read').\")\n",
        "\n",
        "else:\n",
        "    # This block runs if user_token is empty (typically on the first execution)\n",
        "    print(\"\\nNo token detected in the input box.\")\n",
        "    print(\"Please paste your token above and run this cell again to log in and list models.\")\n",
        "\n",
        "# You can optionally unset the widget's value for added security after processing\n",
        "# token_input.value = '' # Uncomment if you want to clear the box after use"
      ],
      "metadata": {
        "id": "rliTHBwEtQ07",
        "outputId": "e064044b-22f3-455e-c75f-047a47700a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An HTTP error occurred: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models?filter=diffusers&pipeline_tag=text-to-image&search=inpaint&sort=lastModified&direction=-1 (Request ID: Root=1-681169d6-46b72a3a39d9567348f59ee0;9d40ccda-5787-4064-8f9f-93cda1f922b2)\n",
            "\n",
            "Invalid credentials in Authorization header\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üé® All-in-One IOpaint with Ngrok\n",
        "\n",
        "#@markdown ## üîë Ngrok Configuration\n",
        "#@markdown Enter your [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken) below:\n",
        "ngrok_token = \"2tjxIXifSaGR3dMhkvhk6sZqbGo_6ZfBZLZHMbtAjfRmfoDW5\" #@param {type:\"string\"}\n",
        "save_token = True #@param {type:\"boolean\"}\n",
        "token_file = \"/content/ngrok_token\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## üì¶ Model Selection\n",
        "#@markdown ### Checkpoint Models (comma-separated list)\n",
        "checkpoint_models = \"lama, sam2_base, gfpgan\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Diffusers Models (comma-separated list of HF repos)\n",
        "diffusers_models = \"briaai/BRIA-2.3-Inpainting, Peixia/sdxl_inpainting_controlnet, Whenzi/Inpaint_carrot_crack, mrcuddle/URPM-Inpaint-Hyper-SDXL, mrcuddle/URPM-Inpaint-SDXL, andro-flock/LUSTIFY-SDXL-NSFW-checkpoint-v2-0-INPAINTING, andro-flock/Clarity-Clarity-v3-inpainting, andro-flock/Liberty-LibertyMain-Inpainting,\" #@param {type:\"string\"}\n",
        "use_fp16_for_diffusers = True #@param {type:\"boolean\"}\n",
        "pre_download_diffusers = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## üöÄ IOpaint Options\n",
        "selected_model = \"urpm_inpaint_sdxl\" #@param [\"lama\", \"mat\", \"migan\", \"sdxl_turbo\", \"urpm_inpaint_sdxl\"]\n",
        "enable_interactive_seg = True #@param {type:\"boolean\"}\n",
        "enable_remove_bg = False #@param {type:\"boolean\"}\n",
        "enable_realesrgan = False #@param {type:\"boolean\"}\n",
        "enable_gfpgan = True #@param {type:\"boolean\"}\n",
        "disable_nsfw = True #@param {type:\"boolean\"}\n",
        "port = 8080 #@param {type:\"integer\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "# Current timestamp\n",
        "print(f\"Current Date: 2025-04-29 22:10:41 UTC\")\n",
        "print(f\"User: remphan1618\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q iopaint pyngrok huggingface_hub gfpgan rembg onnxruntime\n",
        "print(\"‚úÖ Required packages installed\")\n",
        "\n",
        "# Define base directories - Using /content/ for user accessibility\n",
        "BASE_MODEL_DIR = \"/content/iopaint/models\"\n",
        "TORCH_HUB_DIR = os.path.join(BASE_MODEL_DIR, \"torch/hub/checkpoints\")\n",
        "os.makedirs(BASE_MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(TORCH_HUB_DIR, exist_ok=True)\n",
        "\n",
        "# Create a symlink from default location to our accessible location\n",
        "default_model_dir = \"/root/.cache/iopaint/models\"\n",
        "os.makedirs(os.path.dirname(default_model_dir), exist_ok=True)\n",
        "if os.path.exists(default_model_dir):\n",
        "    if os.path.islink(default_model_dir):\n",
        "        os.unlink(default_model_dir)\n",
        "    else:\n",
        "        import shutil\n",
        "        print(f\"Moving existing models from {default_model_dir} to {BASE_MODEL_DIR}...\")\n",
        "        for item in os.listdir(default_model_dir):\n",
        "            src = os.path.join(default_model_dir, item)\n",
        "            dst = os.path.join(BASE_MODEL_DIR, item)\n",
        "            if not os.path.exists(dst):\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        shutil.rmtree(default_model_dir)\n",
        "\n",
        "# Create symlink\n",
        "os.symlink(BASE_MODEL_DIR, default_model_dir)\n",
        "print(f\"‚úÖ Models directory set to {BASE_MODEL_DIR} (accessible via /content/)\")\n",
        "print(f\"‚úÖ Created symlink from {default_model_dir} to {BASE_MODEL_DIR}\")\n",
        "\n",
        "# ========== HANDLE NGROK TOKEN ==========\n",
        "# Load token from file if exists\n",
        "if not ngrok_token and os.path.exists(token_file):\n",
        "    with open(token_file, 'r') as f:\n",
        "        ngrok_token = f.read().strip()\n",
        "    print(f\"‚úÖ Loaded Ngrok token from {token_file}\")\n",
        "\n",
        "# Save token if requested\n",
        "if ngrok_token and save_token:\n",
        "    with open(token_file, 'w') as f:\n",
        "        f.write(ngrok_token)\n",
        "    os.chmod(token_file, 0o600)  # Set permissions to user-only\n",
        "    print(f\"‚úÖ Saved Ngrok token to {token_file}\")\n",
        "\n",
        "if not ngrok_token:\n",
        "    print(\"‚ùå ERROR: Ngrok token is required!\")\n",
        "    print(\"Please get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ========== CHECKPOINT MODEL DOWNLOADER ==========\n",
        "# Checkpoint models database\n",
        "CHECKPOINT_MODELS = {\n",
        "    \"lama\": {\n",
        "        \"url\": \"https://github.com/Sanster/models/releases/download/add_big_lama/big-lama.pt\",\n",
        "        \"filename\": \"big-lama.pt\",\n",
        "        \"size_mb\": 196,\n",
        "        \"description\": \"Default fast inpainting model\"\n",
        "    },\n",
        "    \"mat\": {\n",
        "        \"url\": \"https://github.com/Sanster/models/releases/download/mat/Places_512_FullData_G.pth\",\n",
        "        \"filename\": \"Places_512_FullData_G.pth\",\n",
        "        \"size_mb\": 675,\n",
        "        \"description\": \"Better quality than LaMa but slower\"\n",
        "    },\n",
        "    \"sam2_base\": {\n",
        "        \"url\": \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\",\n",
        "        \"filename\": \"sam2_hiera_base_plus.pt\",\n",
        "        \"size_mb\": 309,\n",
        "        \"description\": \"SAM2 model for interactive segmentation\"\n",
        "    },\n",
        "    \"gfpgan\": {\n",
        "        \"url\": \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\",\n",
        "        \"filename\": \"GFPGANv1.3.pth\",\n",
        "        \"size_mb\": 332,\n",
        "        \"description\": \"Face restoration model\"\n",
        "    },\n",
        "    \"migan\": {\n",
        "        \"url\": \"https://github.com/Sanster/models/releases/download/migan/migan_patch.pt\",\n",
        "        \"filename\": \"migan_patch.pt\",\n",
        "        \"size_mb\": 395,\n",
        "        \"description\": \"Another good inpainting model\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def download_checkpoint(url, dest_path):\n",
        "    \"\"\"Download a file with progress bar\"\"\"\n",
        "    if os.path.exists(dest_path):\n",
        "        file_size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n",
        "        print(f\"‚úÖ Already exists: {os.path.basename(dest_path)} ({file_size_mb:.1f} MB)\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "\n",
        "            with open(dest_path, 'wb') as f, tqdm(\n",
        "                desc=os.path.basename(dest_path),\n",
        "                total=total_size,\n",
        "                unit='B',\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    size = f.write(chunk)\n",
        "                    bar.update(size)\n",
        "\n",
        "        if os.path.exists(dest_path) and os.path.getsize(dest_path) > 0:\n",
        "            size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n",
        "            print(f\"‚úÖ Downloaded: {os.path.basename(dest_path)} ({size_mb:.1f} MB)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to download: {dest_path}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Process checkpoint models\n",
        "if checkpoint_models:\n",
        "    model_list = [m.strip() for m in checkpoint_models.split(\",\") if m.strip()]\n",
        "    print(f\"\\nüì¶ Downloading {len(model_list)} checkpoint models: {', '.join(model_list)}\")\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if model_name in CHECKPOINT_MODELS:\n",
        "            print(f\"\\nüì• Downloading {model_name}: {CHECKPOINT_MODELS[model_name]['description']}\")\n",
        "            dest_path = os.path.join(TORCH_HUB_DIR, CHECKPOINT_MODELS[model_name]['filename'])\n",
        "            download_checkpoint(CHECKPOINT_MODELS[model_name]['url'], dest_path)\n",
        "        else:\n",
        "            print(f\"‚ùå Unknown model: {model_name}\")\n",
        "\n",
        "# ========== DIFFUSERS MODEL DOWNLOADER ==========\n",
        "def sanitize_name(repo_id):\n",
        "    \"\"\"Create a valid model name from repository ID\"\"\"\n",
        "    name = repo_id.split(\"/\")[-1].lower()\n",
        "    name = name.replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\")\n",
        "    if len(name) > 30:\n",
        "        name = name[:30]\n",
        "    return name\n",
        "\n",
        "def setup_diffusers_model(repo_id, use_fp16=False, pre_download=False):\n",
        "    \"\"\"Set up a diffusers model by creating model_info.json\"\"\"\n",
        "    model_name = sanitize_name(repo_id)\n",
        "    model_dir = os.path.join(BASE_MODEL_DIR, model_name)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Create model_info.json\n",
        "    config = {\n",
        "        \"name\": model_name,\n",
        "        \"repo_id\": repo_id,\n",
        "        \"subfolder\": None,\n",
        "        \"type\": \"diffusers\",\n",
        "        \"fp16\": use_fp16,\n",
        "        \"revision\": \"main\",\n",
        "        \"use_checkpoint\": False  # Force diffusers format only\n",
        "    }\n",
        "\n",
        "    config_path = os.path.join(model_dir, \"model_info.json\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Set up model: {model_name}\")\n",
        "    print(f\"  ‚Ä¢ Repository: {repo_id}\")\n",
        "    print(f\"  ‚Ä¢ Using FP16: {'Yes' if use_fp16 else 'No'}\")\n",
        "\n",
        "    # Optionally pre-download the model\n",
        "    if pre_download:\n",
        "        try:\n",
        "            print(f\"  ‚Ä¢ Pre-downloading diffusers components...\")\n",
        "\n",
        "            # Import huggingface_hub\n",
        "            from huggingface_hub import snapshot_download\n",
        "\n",
        "            # Only download diffusers components\n",
        "            allow_patterns = [\n",
        "                \"*.json\",\n",
        "                \"scheduler/**\",\n",
        "                \"text_encoder/**\",\n",
        "                \"text_encoder_2/**\",\n",
        "                \"tokenizer/**\",\n",
        "                \"unet/**\",\n",
        "                \"vae/**\",\n",
        "                \"safety_checker/**\",\n",
        "                \"feature_extractor/**\"\n",
        "            ]\n",
        "\n",
        "            # Skip checkpoint files\n",
        "            ignore_patterns = [\"*.safetensors\", \"*.ckpt\", \"*.bin\", \"*.pt\"]\n",
        "\n",
        "            # Download with progress tracking\n",
        "            print(f\"    ‚è≥ Downloading from {repo_id}...\")\n",
        "            snapshot_download(\n",
        "                repo_id=repo_id,\n",
        "                local_dir=model_dir,\n",
        "                allow_patterns=allow_patterns,\n",
        "                ignore_patterns=ignore_patterns\n",
        "            )\n",
        "            print(f\"    ‚úÖ Download complete!\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error pre-downloading: {str(e)}\")\n",
        "            print(f\"    ‚ÑπÔ∏è Model will download when first used in IOpaint.\")\n",
        "    else:\n",
        "        print(f\"  ‚Ä¢ Model will download when first used in IOpaint\")\n",
        "\n",
        "    return model_name\n",
        "\n",
        "# Process diffusers models\n",
        "configured_diffusers = []\n",
        "if diffusers_models:\n",
        "    model_repos = [m.strip() for m in diffusers_models.split(\",\") if m.strip()]\n",
        "    print(f\"\\nüß© Setting up {len(model_repos)} diffusers models\")\n",
        "\n",
        "    for repo_id in model_repos:\n",
        "        print(f\"\\nüì• Processing: {repo_id}\")\n",
        "        model_name = setup_diffusers_model(repo_id, use_fp16_for_diffusers, pre_download_diffusers)\n",
        "        configured_diffusers.append(model_name)\n",
        "        time.sleep(0.5)  # Small delay between setups\n",
        "\n",
        "# ========== NGROK & IOPAINT LAUNCHER ==========\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Clear any existing tunnels\n",
        "active_tunnels = ngrok.get_tunnels()\n",
        "for tunnel in active_tunnels:\n",
        "    ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "# Set auth token\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Create tunnel\n",
        "url = ngrok.connect(addr=port, bind_tls=True)\n",
        "print(f\"\\nüåé IOPaint URL: {url}\")\n",
        "\n",
        "# Display clickable link and file explorer info\n",
        "display(HTML(f'''\n",
        "<div style=\"background-color: #f0f5ff; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
        "  <h3>üé® IOpaint is Ready!</h3>\n",
        "  <p><b>Access URL:</b> <a href=\"{url}\" target=\"_blank\">{url}</a></p>\n",
        "  <p><b>Models folder:</b> <code>/content/iopaint/models</code> (accessible in file explorer)</p>\n",
        "  <p><b>Selected model:</b> <code>{selected_model}</code></p>\n",
        "</div>\n",
        "'''))\n",
        "\n",
        "# Build IOPaint command\n",
        "cmd = [\n",
        "    'iopaint', 'start',\n",
        "    '--model', selected_model,\n",
        "    '--device', 'cuda',\n",
        "    '--port', str(port),\n",
        "    '--host', '0.0.0.0'\n",
        "]\n",
        "\n",
        "# Add optional parameters\n",
        "if enable_interactive_seg:\n",
        "    cmd.extend(['--enable-interactive-seg', '--interactive-seg-device', 'cuda'])\n",
        "\n",
        "if enable_remove_bg:\n",
        "    cmd.append('--enable-remove-bg')\n",
        "\n",
        "if enable_realesrgan:\n",
        "    cmd.extend(['--enable-realesrgan', '--realesrgan-device', 'cuda'])\n",
        "\n",
        "if enable_gfpgan:\n",
        "    cmd.extend(['--enable-gfpgan', '--gfpgan-device', 'cuda'])\n",
        "\n",
        "if disable_nsfw:\n",
        "    cmd.append('--disable-nsfw-checker')\n",
        "\n",
        "print(\"\\nüöÄ Launching IOPaint with command:\")\n",
        "print(' '.join(cmd))\n",
        "print(\"\\n‚è≥ Please wait until you see 'Application startup complete'...\\n\")\n",
        "\n",
        "# Run the process and keep it alive\n",
        "async def run_process():\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    async def read_stream(stream):\n",
        "        while True:\n",
        "            line = await stream.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            print(line.decode('utf-8').strip())\n",
        "\n",
        "    await asyncio.gather(\n",
        "        read_stream(process.stdout),\n",
        "        read_stream(process.stderr)\n",
        "    )\n",
        "\n",
        "# Run the process\n",
        "try:\n",
        "    await run_process()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping on user request...\")\n",
        "finally:\n",
        "    print(\"Cleaning up Ngrok tunnels...\")\n",
        "    ngrok.kill()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WappSzfitUuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}